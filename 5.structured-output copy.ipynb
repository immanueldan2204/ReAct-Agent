{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain huggingface-hub[cli] langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API key.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import langchain_huggingface python package. Please install it with `pip install langchain_huggingface`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InMemoryStore\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create store with semantic search enabled\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m embeddings = \u001b[43minit_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface:jina-embeddings-v3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m store = InMemoryStore(\n\u001b[32m      7\u001b[39m     index={\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33membed\u001b[39m\u001b[33m\"\u001b[39m: embeddings,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdims\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1024\u001b[39m,\n\u001b[32m     10\u001b[39m     }\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ReAct-Agent/venv/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py:110\u001b[39m, in \u001b[36mbeta.<locals>.beta.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    109\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ReAct-Agent/venv/lib/python3.12/site-packages/langchain/embeddings/base.py:183\u001b[39m, in \u001b[36minit_embeddings\u001b[39m\u001b[34m(model, provider, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m provider, model_name = _infer_model_and_provider(model, provider=provider)\n\u001b[32m    182\u001b[39m pkg = _SUPPORTED_PROVIDERS[provider]\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43m_check_pkg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider == \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ReAct-Agent/venv/lib/python3.12/site-packages/langchain/embeddings/base.py:113\u001b[39m, in \u001b[36m_check_pkg\u001b[39m\u001b[34m(pkg)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check if a package is installed.\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m util.find_spec(pkg):\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    114\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: Could not import langchain_huggingface python package. Please install it with `pip install langchain_huggingface`"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Create store with semantic search enabled\n",
    "embeddings = init_embeddings(\"huggingface:jina-embeddings-v3\")\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1024,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store some memories\n",
    "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
    "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I prefer Italian food\"})\n",
    "store.put((\"user_123\", \"memories\"), \"3\", {\"text\": \"I don't like spicy food\"})\n",
    "store.put((\"user_123\", \"memories\"), \"3\", {\"text\": \"I am studying econometrics\"})\n",
    "store.put((\"user_123\", \"memories\"), \"3\", {\"text\": \"I am a plumber\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I prefer Italian food (similarity: 0.46499372074204315)\n",
      "Memory: I love pizza (similarity: 0.355161890589276)\n",
      "Memory: I am a plumber (similarity: 0.15564700305461443)\n"
     ]
    }
   ],
   "source": [
    "# Find memories about food preferences\n",
    "memories = store.search((\"user_123\", \"memories\"), query=\"I like food?\", limit=5)\n",
    "\n",
    "for memory in memories:\n",
    "    print(f'Memory: {memory.value[\"text\"]} (similarity: {memory.score})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you in the mood for? Since you love Italian food and pizza, would you like to have some pizza, or is there another dish you're craving?"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def chat(state, *, store: BaseStore):\n",
    "    # Search based on user's last message\n",
    "    items = store.search(\n",
    "        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n",
    "    )\n",
    "    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n",
    "            *state[\"messages\"],\n",
    "        ]\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(chat)\n",
    "builder.add_edge(START, \"chat\")\n",
    "graph = builder.compile(store=store)\n",
    "\n",
    "for message, metadata in graph.stream(\n",
    "    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
